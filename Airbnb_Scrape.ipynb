{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver import ActionChains\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create List of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get listings from URL\n",
    "from re import U\n",
    "\n",
    "\n",
    "def get_listings(search_page):\n",
    "    answer = requests.get(search_page, timeout=5)\n",
    "    content = answer.content\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    #get all tags with link\n",
    "    listings = soup.findAll(\"a\", {\"class\": \"rfexzly dir dir-ltr\"})\n",
    "\n",
    "    url_list=[]\n",
    "\n",
    "    #loop through URLs and append to list\n",
    "    for listing in listings:\n",
    "        href = listing.get('href')\n",
    "        url_list.append(\"https://www.airbnb.com\"+href)\n",
    "\n",
    "    #remove duplicate URLs\n",
    "    urls_final = set(url_list)\n",
    "\n",
    "    return urls_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Input Search URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define search URL\n",
    "#any weekend in October\n",
    "airbnb_url = 'https://www.airbnb.com/s/Louisville--Kentucky--United-States/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&place_id=ChIJCROGK48KaYgR_WSJcx2vTro&date_picker_type=flexible_dates&source=structured_search_input_header&search_type=filter_change&ne_lat=38.36561789617469&ne_lng=-85.41682148207042&sw_lat=38.03430121066999&sw_lng=-85.89472675550792&zoom=11&search_by_map=true&flexible_trip_lengths%5B%5D=weekend_trip&flexible_trip_dates%5B%5D=october&query=Louisville%2C%20Kentucky%2C%20United%20States&price_filter_input_type=0&price_min=351'\n",
    "\n",
    "#AirBnB only allows 300 listings in result\n",
    "\n",
    "#1st URL - $10 to $100\n",
    "#2nd URL - $101 to $130\n",
    "#3rd URL - $131 to $160\n",
    "#4th URL - $161 to $210\n",
    "#5th URL - $211 to $250\n",
    "#6th URL - $251 to $350\n",
    "#7th URL - $351 to $1000+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get listings\n",
    "listings = get_listings(airbnb_url)\n",
    "\n",
    "#count listing\n",
    "listings_count = len(listings)\n",
    "print(\"Total listings: \"+str(listings_count))\n",
    "\n",
    "for l in listings:\n",
    "    print(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Determine the max number of pages for given URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "answer = requests.get(airbnb_url, timeout=5)\n",
    "content = answer.content\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "num_list = []\n",
    "\n",
    "#get page numbers\n",
    "page_numbers = soup.findAll(\"a\", {\"class\": \"_833p2h\"})\n",
    "\n",
    "for pn in page_numbers:\n",
    "    num = pn.getText()\n",
    "    num_list.append(int(num))\n",
    "    \n",
    "\n",
    "max_page = max(num_list)\n",
    "print(max_page)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Get URLs from each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to build list of URLs for each page\n",
    "def build_urls(main_url, max_page, listings_per_page=20):\n",
    "    url_list = []\n",
    "    for i in range(max_page):\n",
    "        offset = listings_per_page * i\n",
    "        url_pagination = main_url + f'&items_offset={offset}'\n",
    "        url_list.append(url_pagination)\n",
    "    \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = build_urls(airbnb_url, max_page)\n",
    "\n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create one combined list of URLs to sort through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_count=0\n",
    "master_list = []\n",
    "\n",
    "#function to combine all URLs together\n",
    "def get_all_pages(url_list_all):\n",
    "    for l in url_list_all:\n",
    "        master_list.append(get_listings(l))\n",
    "\n",
    "        global listing_count\n",
    "\n",
    "        #count listings\n",
    "        listing_count = len(listings) + listing_count\n",
    "        print(\"Total listings: \"+str(listing_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_pages(url_list)\n",
    "\n",
    "#print(master_list)\n",
    "\n",
    "count=0\n",
    "\n",
    "for l in master_list:\n",
    "    count += len(l)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Extract all the details from each listing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ast import parse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "#function to get features from each URL\n",
    "def extract_features(url):\n",
    "\n",
    "    global df\n",
    "\n",
    "    title = \"Error\"\n",
    "    price = \"Error\"\n",
    "    gbb = \"Error\"\n",
    "    reviews = \"Error\"\n",
    "    parse_coord = \"Error\"\n",
    "    days_free = \"Error\"\n",
    "    days_booked = \"Error\"\n",
    "\n",
    "    options = Options()\n",
    "    #options.add_argument('--blink-settings=imagesEnabled=false') #need images for Google coordinates\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    #time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        #get title\n",
    "        title = WebDriverWait(driver, 2).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"#site-content > div > div:nth-child(1) > div:nth-child(3) > div > div._16e70jgn > div > div:nth-child(1) > div > div > section > div > div > div > div._tqmy57 > div > h2\")))\n",
    "        title = title.text\n",
    "        #print(title)\n",
    "        \n",
    "        #get price\n",
    "        price = WebDriverWait(driver, 2).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"#site-content > div > div:nth-child(1) > div:nth-child(3) > div > div._1s21a6e2 > div > div > div:nth-child(1) > div > div > div > div > div > div > div._wgmchy > div._c7v1se > div:nth-child(1) > div > span > div > span._tyxjp1\")))\n",
    "        price = price.text\n",
    "        #print(price)\n",
    "        \n",
    "        #get guest count, bedrooms, bathrooms\n",
    "        gbb = WebDriverWait(driver, 2).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"#site-content > div > div:nth-child(1) > div:nth-child(3) > div > div._16e70jgn > div > div:nth-child(1) > div > div > section > div > div > div > div._tqmy57 > ol\")))\n",
    "        gbb = gbb.text\n",
    "        #print(gbb)\n",
    "\n",
    "        days_list = []\n",
    "        days_booked = 0\n",
    "        days_free = 0\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0,2000);\")\n",
    "\n",
    "        #get number of days booked and free\n",
    "        for row in WebDriverWait(driver, 2).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"_cvkwaj\"))):\n",
    "            for r in row.find_elements(By.TAG_NAME, 'tr'):\n",
    "                for cell in r.find_elements(By.TAG_NAME, 'td'):\n",
    "                    \n",
    "                    outerHTML = cell.get_attribute(\"outerHTML\")\n",
    "                    #print(outerHTML)\n",
    "                    pattern = r'(data-is-day-blocked=\"true\")'\n",
    "\n",
    "                    try:  \n",
    "                        is_day_booked = bool(re.search(pattern, outerHTML).group(1))\n",
    "                    except AttributeError:\n",
    "                        is_day_booked = \"False\"\n",
    "\n",
    "                    if cell.text.isdigit() and is_day_booked == True:\n",
    "                        days_list.append(\"Day \"+cell.text+\" - \"+str(is_day_booked))\n",
    "                        days_booked += 1\n",
    "                    elif cell.text.isdigit() and is_day_booked == \"False\":\n",
    "                        days_list.append(\"Day \"+cell.text+\" - \"+str(is_day_booked))\n",
    "                        days_free += 1\n",
    "                    else: \n",
    "                        cell_new = re.sub(\"[^0-9]\", \"\", cell.text)\n",
    "                        if cell_new.isdigit() and is_day_booked == True:\n",
    "                            days_list.append(\"Day \"+cell_new+\" - \"+str(is_day_booked))\n",
    "                            days_booked += 1\n",
    "                        elif cell_new.isdigit() and is_day_booked == \"False\":\n",
    "                            days_list.append(\"Day \"+cell_new+\" - \"+str(is_day_booked))\n",
    "                            days_free += 1\n",
    "\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0,3500);\")\n",
    "        \n",
    "        #get coordinates from Google maps\n",
    "        coordinates = WebDriverWait(driver, 3).until(EC.visibility_of_element_located((By.XPATH, \"//a[contains(@href,'https://maps.google.com/maps?ll=')]\")))\n",
    "        coord_href = str(coordinates.get_attribute('href'))\n",
    "\n",
    "        parse_coord = re.findall(r\"ll=(-?\\d+\\.\\d+),(-?\\d+\\.\\d+)\", coord_href)\n",
    "\n",
    "        parse_coord = parse_coord[0]\n",
    "        # print(parse_coord)\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0,2200);\")\n",
    "\n",
    "        #get reviews\n",
    "        reviews = WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"#site-content > div > div:nth-child(1) > div:nth-child(4) > div > div > div > div:nth-child(2) > section > div.h1v4rb5q.dir.dir-ltr > span.t1xdm4l6.dir.dir-ltr > h2 > div > span\")))\n",
    "        if type(reviews) != str:\n",
    "            reviews = reviews.text\n",
    "        #print(reviews)\n",
    "\n",
    "    except NoSuchElementException: \n",
    "        pass\n",
    "\n",
    "    except StaleElementReferenceException:\n",
    "        pass\n",
    "\n",
    "    except TimeoutException:\n",
    "        reviews = \"No Reviews Yet\"\n",
    "        pass\n",
    "   \n",
    "\n",
    "    # print(days_list)\n",
    "    # print(\"Days Booked: \"+str(days_booked))\n",
    "    # print(\"Days Free: \"+str(days_free))\n",
    "\n",
    "    #add variables to DataFrame\n",
    "    new_row = pd.DataFrame([[title,price,gbb,reviews,str(parse_coord),days_free,days_booked,url]],columns=['title','price','gbb','reviews','coordinates','days_free','days_booked','url'])\n",
    "\n",
    "    #append row to DataFrame\n",
    "    df = df.append(new_row)\n",
    "\n",
    "    #display(df)\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Insert Dataframe to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert DataFrame rows to MySQL\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "engine = create_engine(\n",
    "  'mysql+pymysql://{user}:{password}@{account}'.format(\n",
    "    #removed creds\n",
    "    user='',\n",
    "    password=parse.quote(''),\n",
    "    account='',\n",
    "  ) \n",
    ")\n",
    "\n",
    "#time to extract some features\n",
    "\n",
    "for urls in master_list:\n",
    "    for page in urls:\n",
    "        print(page)\n",
    "        print(len(df.index))\n",
    "\n",
    "        extract_features(page)\n",
    "\n",
    "\n",
    "df.to_sql(\"airbnb_listings\", if_exists='append', index=False, con=engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Reverse Geocode coordinates to get Zip Code with geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy\n",
    "import pandas as pd\n",
    "import csv \n",
    "from geopy.point import Point\n",
    "\n",
    "\n",
    "geolocator = geopy.Nominatim(user_agent=\"airbnb\")\n",
    "\n",
    "#removed file path\n",
    "with open('/') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        record = []\n",
    "        lat=row['latitude']\n",
    "        lon=row['longitude']\n",
    "        url=row['url']\n",
    "        location = geolocator.reverse(Point(lat,lon))\n",
    "        print(location)\n",
    "\n",
    "        try:\n",
    "            zip = location.raw['address']['postcode']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        record.append(lat)\n",
    "        record.append(lon)\n",
    "        record.append(url)\n",
    "        record.append(zip)\n",
    "        print(record)\n",
    "\n",
    "        #removed file path\n",
    "        with open('', 'a', newline='', encoding='utf-8') as op_file:\n",
    "            wr = csv.writer(op_file)\n",
    "            wr.writerow(record)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
